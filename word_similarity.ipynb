{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !yes|pip uninstall spacy\n",
        "# !pip install spacy==3.1.1\n",
        "# !python -m spacy info\n",
        "import spacy\n",
        "print(spacy.__version__)  # 3.1.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y12SmsipDGQe",
        "outputId": "3f1f3bdc-4580-4eae-ae85-c04955e4059e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!yes | pip uninstall spacy\n",
        "\n",
        "!python -m spacy info\n",
        "import spacy\n",
        "print(spacy.__version__)  # 3.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a95NP5AH6HJ",
        "outputId": "a13ec179-688e-489c-e2e0-41fad3554448"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-08 12:57:38.910386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.5.2                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-5.15.107+-x86_64-with-glibc2.31\n",
            "Python version   3.10.11                       \n",
            "Pipelines        en_core_web_sm (3.5.0)        \n",
            "\n",
            "3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "!yes|pip uninstall spacy-transformers\n",
        "!pip install spacy-transformers\n",
        "\n",
        "LANG = \"ITA\" # \"ENG\" or \"ITA\"\n",
        "\n",
        "if LANG == \"ITA\":\n",
        "  !python -m spacy download it_core_news_lg\n",
        "  nlp = spacy.load(\"it_core_news_lg\")\n",
        "  import it_core_news_lg\n",
        "  pass\n",
        "elif LANG == \"ENG\":\n",
        "  !python -m spacy download en_core_web_lg --force\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "  import en_core_web_lg\n",
        "  nlp = en_core_web_lg.load()\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lh7uxzhh28bA",
        "outputId": "055d7bea-a848-4861-f46d-ca3b414552dd"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: spacy-transformers 1.2.4\n",
            "Uninstalling spacy-transformers-1.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/spacy_transformers-1.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/spacy_transformers/*\n",
            "Proceed (Y/n)?   Successfully uninstalled spacy-transformers-1.2.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy-transformers\n",
            "  Using cached spacy_transformers-1.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (1.22.4)\n",
            "Requirement already satisfied: transformers<4.30.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (4.29.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.4.6)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.30.0,>=3.4.0->spacy-transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Installing collected packages: spacy-transformers\n",
            "Successfully installed spacy-transformers-1.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy_transformers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-08 15:02:42.438821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting it-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.5.0/it_core_news_lg-3.5.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from it-core-news-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_transformers\n",
        "import numpy as np\n",
        "\n",
        "doc = nlp(\"Frase italiana.\")\n",
        "\n",
        "print([(w.text, w.pos_) for w in doc])\n",
        "print(len(nlp.vocab))"
      ],
      "metadata": {
        "id": "jW8J5k-3mgb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290f8786-aad5-46b7-d643-360d9fb7d8d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Frase', 'NOUN'), ('italiana', 'ADJ'), ('.', 'PUNCT')]\n",
            "383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "word1 = nlp('nero')\n",
        "word2 = nlp('bianco')\n",
        "\n",
        "similarity = word1.similarity(word2)\n",
        "print(f'{word1} ~= {word2} {similarity}') # 0.835\n",
        "\n",
        "print(len(nlp.vocab.strings))\n",
        "list(sorted(random.choices(list(nlp.vocab.strings), k=10)))"
      ],
      "metadata": {
        "id": "nw2-AxkkmO9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e202c0-30a7-48c8-b286-8080f5dbd019"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nero ~= bianco 0.8355581480087845\n",
            "681834\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4325',\n",
              " 'Aprica',\n",
              " 'Dubai',\n",
              " 'LHI',\n",
              " 'Montato',\n",
              " 'artistique',\n",
              " 'ballonzola',\n",
              " 'fablier',\n",
              " 'i)1',\n",
              " 'schillaci']"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True)\n",
        "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
        "    assert(u.shape[0] == v.shape[0])\n",
        "    uv = 0\n",
        "    uu = 0\n",
        "    vv = 0\n",
        "    for i in range(u.shape[0]):\n",
        "        uv += u[i]*v[i]\n",
        "        uu += u[i]*u[i]\n",
        "        vv += v[i]*v[i]\n",
        "    cos_theta = 1\n",
        "    if uu != 0 and vv != 0:\n",
        "        cos_theta = uv/np.sqrt(uu*vv)\n",
        "    return cos_theta"
      ],
      "metadata": {
        "id": "1w7vaiY2p0z1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar(word, topn=5, reverse=True):\n",
        "  word = nlp.vocab[str(word)]\n",
        "  queries = [\n",
        "      w for w in nlp.vocab \n",
        "      if np.count_nonzero(w.vector)\n",
        "  ]\n",
        "\n",
        "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=reverse)\n",
        "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
        "\n",
        "similar = most_similar(\"salume\", topn=3)\n",
        "print(similar)\n",
        "\n",
        "\n",
        "dir(nlp.vocab[\"king\"])\n",
        "nlp.vocab[\"king\"].has_vector\n",
        "nlp"
      ],
      "metadata": {
        "id": "8KubftWNp1g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dd4dac-34d6-46cf-f0a6-459e47dd5632"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('caserecci', 1.0), ('norcini', 1.0), ('salumieri', 1.0), ('casarecci', 1.0)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.it.Italian at 0x7f57ecb02320>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance"
      ],
      "metadata": {
        "id": "8l1DNHLCaL9F"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar2(word, topn=5, distanceFunction=None, reverse=True):\n",
        "  word = nlp.vocab[str(word)]\n",
        "  queries = [\n",
        "      w for w in nlp.vocab \n",
        "      if np.count_nonzero(w.vector)\n",
        "  ]\n",
        "\n",
        "  if distanceFunction is None:\n",
        "    distanceFunction = cosine_similarity_numba\n",
        "\n",
        "  by_similarity = sorted(queries, key=lambda w: distanceFunction(word.vector, w.vector), reverse=reverse)\n",
        "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
        "\n",
        "similar = most_similar2(\"salume\", distanceFunction=distance.russellrao, reverse=False)\n",
        "print(similar)\n",
        "\n",
        "\n",
        "dir(nlp.vocab[\"king\"])\n",
        "nlp.vocab[\"king\"].has_vector\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayaGyDQsL96f",
        "outputId": "c0c08e72-31dd-4660-a5e3-0ab2988288fc"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('mosti', 0.6962182521820068), ('spumanti', 0.6962182521820068), ('enoici', 0.6962182521820068), ('crus', 0.6962182521820068), ('vini', 0.6962182521820068), ('vignaioli', 0.6962182521820068)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.it.Italian at 0x7f57ecb02320>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# cosine similarity\n",
        "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "\n",
        "def most_similar3(word, topn=5, reverse=True):\n",
        "  word = nlp.vocab[str(word)]\n",
        "  queries = [\n",
        "      w for w in nlp.vocab \n",
        "      if np.count_nonzero(w.vector)\n",
        "  ]\n",
        "\n",
        "  by_similarity = sorted(queries, key=lambda w: cosine(word.vector, w.vector), reverse=reverse)\n",
        "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
        "\n",
        "similar = most_similar3(\"salume\", topn=3)\n",
        "print(similar)\n",
        "\n",
        "\n",
        "dir(nlp.vocab[\"king\"])\n",
        "nlp.vocab[\"king\"].has_vector\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFI-6u8aRW9n",
        "outputId": "f6a58673-2127-4842-cfee-28d96b8c7fb9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('caserecci', 1.0), ('norcini', 1.0), ('salumieri', 1.0), ('casarecci', 1.0)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.it.Italian at 0x7f57ecb02320>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constansts\n",
        "\n",
        "START_WORD_LENGTHS = [4,5,6]\n",
        "\n",
        "MIN_WORD_LENGTH = 3\n",
        "MAX_WORD_LENGTH = 9\n",
        "\n",
        "NUM_STEPS = 8\n",
        "MIN_STEPS = 5\n",
        "\n"
      ],
      "metadata": {
        "id": "iHLr2i4jGyoQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Feb 7, 2013\n",
        "\n",
        "@author: olegs\n",
        "'''\n",
        "\n",
        "ROMAN_CONSTANTS = (\n",
        "            ( \"\", \"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\", \"VIII\", \"IX\" ),\n",
        "            ( \"\", \"X\", \"XX\", \"XXX\", \"XL\", \"L\", \"LX\", \"LXX\", \"LXXX\", \"XC\" ),\n",
        "            ( \"\", \"C\", \"CC\", \"CCC\", \"CD\", \"D\", \"DC\", \"DCC\", \"DCCC\", \"CM\" ),\n",
        "            ( \"\", \"M\", \"MM\", \"MMM\", \"\",   \"\",  \"-\",  \"\",    \"\",     \"\"   ),\n",
        "        )\n",
        "\n",
        "ROMAN_SYMBOL_MAP = dict(I=1, V=5, X=10, L=50, C=100, D=500, M=1000)\n",
        "\n",
        "CUTOFF = 4000\n",
        "BIG_DEC = 2900\n",
        "BIG_ROMAN = \"MMCM\"\n",
        "ROMAN_NOUGHT = \"nulla\"\n",
        "\n",
        "def digits(num):\n",
        "    if num < 0:\n",
        "        raise Exception('range error: negative numbers not supported')\n",
        "    if num % 1 != 0.0:\n",
        "        raise Exception('floating point numbers not supported')\n",
        "    res = []\n",
        "    while num > 0:\n",
        "        res.append(num % 10)\n",
        "        num //= 10\n",
        "    return res\n",
        "\n",
        "def toString(num, emptyZero=False):\n",
        "    if num < CUTOFF:\n",
        "        digitlist = digits(num)\n",
        "        if digitlist:\n",
        "            res = reversed([ ROMAN_CONSTANTS[order][digit] for order, digit in enumerate(digitlist) ])\n",
        "            return \"\".join(res)\n",
        "        else:\n",
        "            return \"\" if emptyZero else ROMAN_NOUGHT \n",
        "    else:\n",
        "        if num % 1 != 0.0:\n",
        "            raise Exception('floating point numbers not supported')\n",
        "        # For numbers over or equal the CUTOFF, the remainder of division by 2900\n",
        "        # is represented as above, prepended with the multiples of MMCM (2900 in Roman),\n",
        "        # which guarantees no more than 3 repetitive Ms.\n",
        "        return BIG_ROMAN * (num // BIG_DEC) + toString(num % BIG_DEC, emptyZero=True)\n",
        "\n",
        "def parse(numeral):\n",
        "    numeral = numeral.upper()\n",
        "    result = 0\n",
        "\n",
        "    lastVal = 0\n",
        "    lastCount = 0\n",
        "    subtraction = False\n",
        "    for symbol in numeral[::-1]:\n",
        "        value = ROMAN_SYMBOL_MAP.get(symbol)\n",
        "        if not value:\n",
        "            return False\n",
        "        if lastVal == 0:\n",
        "            lastCount = 1\n",
        "            lastVal = value\n",
        "        elif lastVal == value:\n",
        "            lastCount += 1\n",
        "            # exceptions\n",
        "        else:\n",
        "            result += (-1 if subtraction else 1) * lastVal * lastCount\n",
        "            subtraction = lastVal > value\n",
        "            lastCount = 1\n",
        "            lastVal = value\n",
        "    \n",
        "    result = result + (-1 if subtraction else 1) * lastVal * lastCount\n",
        "    return True"
      ],
      "metadata": {
        "id": "ECs8D5PzG4G1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "dictionaryFileName = 'assets/dictionary_FULL_IT.txt'\n",
        "dictionaryFileName = 'assets/Ita.txt'\n",
        "\n",
        "    \n",
        "def normalizeWord(word):\n",
        "    return word.strip().upper()\n",
        "    \n",
        "LOAD_FROM_URL = True\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "lines = []\n",
        "if LOAD_FROM_URL:\n",
        "    baseUrl = 'https://azrafe7.github.io/'\n",
        "    f = requests.get(baseUrl + dictionaryFileName)\n",
        "    lines = f.text.splitlines()\n",
        "else:\n",
        "    with open(dictionaryFileName) as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "filteredWords = []\n",
        "for line in lines:\n",
        "    word = normalizeWord(line)\n",
        "    length = len(word)\n",
        "    if length >= MIN_WORD_LENGTH and length <= MAX_WORD_LENGTH and not parse(word):\n",
        "        filteredWords.append(word)\n",
        "\n",
        "print(\"Dictionary loaded in {0:.2f}s\".format(time.time() - t0))\n",
        "\n",
        "print('{0}/{1}'.format(len(filteredWords), len(lines)))\n",
        "\n",
        "print(filteredWords[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzYtS2s6G43t",
        "outputId": "c766204e-ac35-4732-eff1-44b2c35b5efe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary loaded in 0.87s\n",
            "401123/710993\n",
            "['AALENIANA', 'AALENIANE', 'AALENIANI', 'AALENIANO', 'ABABUA', 'ABACA', 'ABACETI', 'ABACETO', 'ABACHI', 'ABACHISTA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in filteredWords:\n",
        "  nlp.vocab[w.lower()]"
      ],
      "metadata": {
        "id": "hMWMScsVHKtd"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "word = 'scrofa'\n",
        "\n",
        "word2 = 'maiale'\n",
        "print(nlp.vocab[word].similarity(nlp.vocab[word2]))\n",
        "print(cosine_similarity_numba(nlp.vocab[word].vector, nlp.vocab[word2].vector))\n",
        "\n",
        "# t0 = time.time()\n",
        "# similars = most_similar(word\n",
        "# print('{0:.2f}s'.format(time.time() - t0))\n",
        "# display(similars)\n",
        "\n",
        "t0 = time.time()\n",
        "similars2 = most_similar2(word, topn=30, distanceFunction=None, reverse=True)\n",
        "print('{0:.2f}s'.format(time.time() - t0))\n",
        "display(similars2)\n",
        "\n",
        "t0 = time.time()\n",
        "similars2 = most_similar2(word, topn=30, distanceFunction=distance.chebyshev, reverse=False)\n",
        "print('{0:.2f}s'.format(time.time() - t0))\n",
        "display(similars2)\n",
        "\n",
        "# t0 = time.time()\n",
        "# similars3 = most_similar3(word, topn=10, reverse=True)\n",
        "# print('{0:.2f}s'.format(time.time() - t0))\n",
        "# display(similars3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "skJDNdhFnRhM",
        "outputId": "872bb933-288a-4ca5-eb80-bae0ecb08b33"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5014864802360535\n",
            "0.5014864654258369\n",
            "2.39s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('scrofe', 0.6642468571662903),\n",
              " ('vacca', 0.6385034918785095),\n",
              " ('mucca', 0.6244244575500488),\n",
              " ('cerva', 0.6015369296073914),\n",
              " ('pecora', 0.5831999778747559),\n",
              " ('capra', 0.5800632834434509),\n",
              " ('lepre', 0.5788684487342834),\n",
              " ('lupa', 0.5679877400398254),\n",
              " ('cervo', 0.5673580765724182),\n",
              " ('gallina', 0.5636082291603088),\n",
              " ('maialina', 0.558182954788208),\n",
              " ('iena', 0.557919979095459),\n",
              " ('capretta', 0.5569721460342407),\n",
              " ('cagna', 0.555469810962677),\n",
              " ('mandria', 0.5463758707046509),\n",
              " ('cervone', 0.5436885356903076),\n",
              " ('bovidi', 0.5420840978622437),\n",
              " ('bestiola', 0.5411077737808228),\n",
              " ('scimmia', 0.5401797890663147),\n",
              " ('ovaiola', 0.5400537848472595),\n",
              " ('bestia', 0.5387506484985352),\n",
              " ('scrofola', 0.5337382555007935),\n",
              " ('sbrana', 0.5306189656257629),\n",
              " ('giovenca', 0.5292507410049438),\n",
              " ('femmina', 0.5281515717506409),\n",
              " ('vitellina', 0.5271637439727783),\n",
              " ('bruca', 0.5271130800247192),\n",
              " ('zanna', 0.5220664739608765),\n",
              " ('gatta', 0.5218215584754944),\n",
              " ('capriolo', 0.5217816233634949)]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.93s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('babbuino', 0.5042772889137268),\n",
              " ('scrofe', 0.6642468571662903),\n",
              " ('miagola', 0.42082837224006653),\n",
              " ('cagnaccio', 0.43688228726387024),\n",
              " ('azzanna', 0.3994207978248596),\n",
              " ('puledra', 0.5144197344779968),\n",
              " ('pollastra', 0.48318392038345337),\n",
              " ('scimmia', 0.5401797890663147),\n",
              " ('capriolo', 0.5217816233634949),\n",
              " ('trisavola', 0.29908379912376404),\n",
              " ('issopo', 0.19020125269889832),\n",
              " ('onagro', 0.2515365779399872),\n",
              " ('lupoide', 0.4403741955757141),\n",
              " ('trisnonna', 0.30644306540489197),\n",
              " ('scrofola', 0.5337382555007935),\n",
              " ('o_o', 0.07505510002374649),\n",
              " ('vitellone', 0.4390723407268524),\n",
              " ('strabuzza', 0.20887790620326996),\n",
              " ('cediglia', 0.2527424693107605),\n",
              " ('guaito', 0.2804921865463257),\n",
              " ('scudiscio', 0.2600635290145874),\n",
              " ('mufloni', 0.413777619600296),\n",
              " ('cagnacci', 0.3986813724040985),\n",
              " ('porcilaia', 0.4753342270851135),\n",
              " ('abomaso', 0.3224746584892273),\n",
              " ('buffalo', 0.36930644512176514),\n",
              " ('bicefala', 0.3548136055469513),\n",
              " ('forzuto', 0.2843242883682251),\n",
              " ('miagolare', 0.3340544104576111),\n",
              " ('cerva', 0.6015369296073914)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'-ci' in nlp.vocab\n",
        "#[x.text for x in nlp.vocab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI6tOcTQJxp2",
        "outputId": "67782d6d-e65b-40d3-9390-532d74d2e47f"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cotto'\n",
        "lexeme = nlp.vocab[word]\n",
        "lexeme.orth_\n",
        "print(lexeme.prob)\n",
        "lexeme.sentiment\n",
        "doc = nlp(word)\n",
        "doc\n",
        "token = doc[0]\n",
        "token\n",
        "token.sentiment\n",
        "print(token.lemma_)\n",
        "print(token.pos_)\n",
        "print(token.morph.to_dict())\n",
        "print(token.morph.get('Gender'))\n"
      ],
      "metadata": {
        "id": "1FKbocLVlYZb",
        "outputId": "09c8dffe-5263-41ee-efb4-ca0000635de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-20.0\n",
            "cotto\n",
            "VERB\n",
            "{'Gender': 'Masc', 'Number': 'Sing', 'Tense': 'Past', 'VerbForm': 'Part'}\n",
            "['Masc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = nlp.vocab['chaos']\n",
        "sorted([x.text for x in nlp.vocab])\n",
        "len([x.text for x in nlp.vocab])\n"
      ],
      "metadata": {
        "id": "PmeB5Hg0qECk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "parser = spacy.load(\"en_core_web_md\")\n"
      ],
      "metadata": {
        "id": "oze69EFDtDoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "word = u\"dog\"\n",
        "nasa = nlp.vocab[word]\n",
        "\n",
        "# cosine similarity\n",
        "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "\n",
        "# gather all known words, take only the lowercased versions\n",
        "allWords = list({w for w in nlp.vocab if w.has_vector and w.orth_.islower() and w.is_alpha and w.lower_ != word})\n",
        "\n",
        "# sort by similarity to NASA\n",
        "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
        "allWords.reverse()\n",
        "print(f\"Top 10 most similar words to {word}:\")\n",
        "for word in allWords[:20]:\n",
        "    print(word.orth_, cosine(nasa.vector, word.vector))\n",
        "\n",
        "#[w.text for w in allWords]"
      ],
      "metadata": {
        "id": "Cs9fQutVuU5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa98fc7f-1811-4442-d0a2-6086ce9601fb"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to dog:\n",
            "dogs 0.7097339\n",
            "baby 0.5822533\n",
            "chicken 0.53386366\n",
            "spots 0.51317126\n",
            "bear 0.5050963\n",
            "bull 0.49388632\n",
            "sled 0.4913776\n",
            "burger 0.48703855\n",
            "burgers 0.47693646\n",
            "hamburger 0.4680303\n",
            "pants 0.46468768\n",
            "cocker 0.4646298\n",
            "beach 0.44747853\n",
            "pony 0.4451662\n",
            "pudding 0.44414398\n",
            "teddy 0.43525335\n",
            "beagle 0.4318026\n",
            "sugar 0.4293324\n",
            "biking 0.41360223\n",
            "truck 0.4121126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def replace_synonyms(text):\n",
        "    doc = nlp(text)\n",
        "    new_doc = []\n",
        "    for token in doc:\n",
        "        print(token.has_vector, token.pos_)\n",
        "        if token.has_vector and token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PROPN\"]:\n",
        "            synonyms = [t for t in nlp.vocab if t.has_vector and t.similarity(token) > 0.8 and t.text != token.text]\n",
        "            if synonyms:\n",
        "                print(token.text, [s.text for s in synonyms])\n",
        "                new_token = random.choice(synonyms)\n",
        "                new_doc.append(new_token.text)\n",
        "            else:\n",
        "                new_doc.append(token.text)\n",
        "        else:\n",
        "            new_doc.append(token.text)\n",
        "    return \" \".join(new_doc)\n",
        "\n",
        "text = \"bere una birra in compagnia\"\n",
        "synonym_replaced_text = replace_synonyms(text)\n",
        "print(synonym_replaced_text)\n"
      ],
      "metadata": {
        "id": "wAZn5veRFUQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8684d17e-2f30-4f25-ce61-5856876dab48"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True VERB\n",
            "True DET\n",
            "True NOUN\n",
            "birra ['birr']\n",
            "True ADP\n",
            "True NOUN\n",
            "bere una birr in compagnia\n"
          ]
        }
      ]
    }
  ]
}